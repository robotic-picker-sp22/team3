<!-- 
LAUNCHES:
- The AR marker code to publish the location of the AR tags
- The shelf frame publisher to use the AR tags to publish a frame 
  for the shelf
- The point cloud demo to crop and identify objects
USAGE:
Running in simulation:
(must already be running roscore and gazebo)
> roslaunch perception perception.launch cam:=mock_point_cloud
Running on the real robot:
default camera (arg cam) should be set to "/head_camera/depth_registered/points"
> roslaunch perception perception.launch
-->
<launch>
	<arg name="cam" default="/head_camera/depth_registered/points"/>
	<arg name="data_dir" default="/home/capstone/catkin_ws/src/fetch-picker/combined_labels"/>
	<param name="ec_cluster_tolerance" value="0.018" />
	<param name="ec_min_cluster_size" value="300" />
	<param name="ec_max_cluster_size" value="20000" />
	<param name="distance_above_plane" value="0.01" />
	<param name="crop_min_x" value="-1" />
	<param name="crop_max_x" value="1" />
	<param name="crop_min_y" value="-1" />
	<param name="crop_max_y" value="1" />
	<param name="crop_min_z" value="-1" />
	<param name="crop_max_z" value="1" />
	<param name="size_weight" value="0.8" />
	
	<include file="$(find robot_api)/launch/ar_desktop.launch">
		<arg name="cam_image_topic" value="$(arg cam)" />
	</include>
	
	<node pkg="perception" type="shelf_frame_publisher.py" name="shelf_frame_publisher" output="screen" />
	
	<node pkg="perception" type="point_cloud_demo" name="point_cloud_demo" args="$(arg data_dir)" output="screen">
		<remap from="cloud_in" to="$(arg cam)" />
	</node>
</launch>